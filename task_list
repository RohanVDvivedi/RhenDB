 PICKED TASKS

 1.* lock table (using VolatilePageStore)
   **** How can I let the lock_manager block the transaction, because it could be running inside a mini_transaction engine's threadpool of fixed size and blocking will block all other thread and possibly not letting the other transaction unblock it by releasing the locks. Additionally I can not let the transaction block before releasing the latches on pages and other structures, and the buffer pool etc and releasing other resources like cursors over the btree index and the heap table, I can not even go to wait before storing the information like the restart position of the scan for the btree or heap table or any other type of scan.
     * block outside, and use a callback to wake the blocking threads up, or queue the task to start a new scan form the tuple position in context in a heap table or btree or hash table
   ** ponder and ask chatgpt on a mechanism to release all latches before going to sleep on a condition variable to wait for a lock, we would also need to store the scan context for the b+tree or heap table scan position
   * decide on when to create and destroy active_transaction_entry, i.e. when to garbage collect
     * store counter to number of threads that went to wait for a transaction_id and the count of locks held, when they both become 0, remove that entry for the active transaction entry
   * decide the way to lock table and other components of the lock_manager
   * implement initialize and register_lock function of the lock_manager
   * outline the pseudocode for the acquire/transition and release function for the lock_manager
   * ponder how to signal about a potential deadlock, to the active_transaction_entry
     * set a bit stating that a deadlock was detected
   * ponder on how to implement release_all_locks function, and is there a way to do it with the existing functions with lesser time complexity?
     * will we need a function to get all locks for a transaction one by one (iterating with the tx_locks table) and wake up all blocked transaction and remove corresponding entries after that?
   * define basic lock martix for row, heap_table (ordering_lock), simple reader writer lock
   * write test cases and test

 2.* define catalog tables and indexes on it - persistent
   * same as indexes and heap_tables
   * prilimnary investigation revealed no-MVCC and no-per-tuple-locking, instead per table locking, do more research

 3.* giant_tuple_def, an in-memory tuple_def for an on-disk tuple_def (containing worms)
   * conversion function to convert on-disk tuple_def to giant_tuple_def
   * conversion function to convert on-disk tuple to giant_tuple
   * use this giant_tuple_def's tuples for in-memory tuples
   * compute max_size for this tuples using tuplestore, and also limit max element_count for the complete project as a macro

 4.* tuple_file structure
   * stores tuples or giant_tuples sequentially one after the another and reads them back like from a stream and deletes this temp file on closing
   * struct intermediate_tuple_store {
        uint64_t in_memory_size; // inserts crossin this value spill to the temp file
        uint64_t file_size;
        union {
          int fd; // temp file, if next_insert_offset > in_memory_size
          void* md;
        }
        uint64_t next_insert_offset;
     }
   * void initializes_intermediate_tuple_store(intermediate_tuple_store* its_p, uint64_t in_memory_size)
   * void* map_intermediate_tuple(intermediate_tuple_store* its_p, uint64_t offset, const tuple_size_def* tpl_sz_d, uint64_t* next_tuple_offset);
     * mmap's pages that contain the tuple and returns the corresponding pointer, MAP_SHARED and MAP_READ | MAP_WRITE
     * pass next_tuple_offset as NULL, if you do not want it calculated, and want to map all pages from offset to the next_insert_offset
   * uint64_t unmap_intermediate_tuple(intermediate_tuple_store* its_p, const void* tuple, const tuple_size_def* tpl_sz_d);
     * munmaps memory, next_insert_offset = max(next_insert_offset, get_tuple_size(tuple, tpl_sz_d));
     * returns next_tuple_offset for this tuple
   * uint64_t extend_intermediate_tuple_store(intermediate_tuple_store* its_p, uint64_t extend_bytes_count);
     * return the bytes to be written from
   * write can also be done as a stream, returning the offset in the file for this tuple
   * file stays all in-memory up until a point, over which it spills to the disk, and then is accessed from there
   * no tuple updates allowed
   * also allows read-only mapping to mmap (MAP_SHARED and read-only mapping) pages containing a tuple, and operate on it as if it is all in memory
   * usefull for external sorting, hash joins and passing intermediate tuples between different operators

 5.0* implement SQL parser (possibly in a different directory)
 5.1* implement query plan tree
   * with operators
 5.2* implement relational algebra operators
   * selection -> filter with/without indexes
   * projection -> picking columns
   * sorting -> implement external giant_tuple_def sorting, by using a tuple_file
   * joins -> with/without indexes (hashjoins using a tuple_file)
   * union, set difference and intersection

 6.* define statistics tables - persistent and possibly in a different database
   * same as indexes and heap_tables (low priority)

 * Design methodology
   * we will use postgresql architecture over (mysql like) mini transactions
   * we will have heaps for storing tables, insert to which gives us a page_id (physical page_id) and a tuple_index (slot no) in that page
   * indexes will always include the page_id, tuple_index in the index key right after the actual key columns, and the index entry will be index key followed by the covering columns
   * standard latch rules apply index->heap_page, heap_pages_tree->heap_page
   * inserts/deletes will always be done to heap in a single mini transaction one at a time
     * inserts/deletes to indexes will be done in mini transactions not owned by anyone
   * updates are just delete followed by insert into the heap
   * reads will be be done without mini transactions, but in a fixed number of them
   * there will be vaccum, like postgresql
   * header will include xmin, xmax, is_xmin_committed?, is_xmax_committed?, is_xmin_aborted?, is_xmax_aborted?
     * xmin, xmax will be as wide as transaction_table suggests
     * the *? are advisory bitmaps and should be kept updated when ever a heap page is being read (if only a read lock on page is held then let another writer do it asynchronously) or written
   * we will never chain mini transactions for a high level transaction, we will just ensure that the transactions read only the data that is visible to them, and vaccum out invisible rows
   * lock_tables -> need to think over it