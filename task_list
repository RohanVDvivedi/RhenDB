 PICKED TASKS

 * solution to lock_table problems
   * remove timeout from acquire_lock, instead pass a flag that says is_non_blocking
   * it has to be controlled by an external lock, and no rwlock for the tables
   * hold the external lock before calling any of it's functions
   * add try_lock also along with the acquire_lock function
     * timeout is no longer returned, deadlock is never detected by this function
   * acquire_lock returns SUCCESS -> (lock acquired, lock transitioned or lock already held), FAILED (for try lock only), DEADLOCK_NOTIFIED, MUST_BLOCK (for acquire lock, wait entries inserted)
   * pass callbacks for
     * notify_unblocked(context_p, transaction_id, task_id);
     * get_transaction_priority(context_p, transaction_id);
     * update_transaction_priority(context, transaction_id, uint64_t new_priority);
     * notify_deadlock(context_p, transaction_id);
     * was_deadlock_notified(context_p, transaction_id);
   * release_lock -> remains as decided earlier, releases the lock and wakes up all the transactions waiting for that lock
   * in the wait_entry store also the task_id a task for a transaction that needs to be unblocked
   * we only unblock the only task that is responsible for the wait
   * any time a task has acquired or released a lock all the wait_entries for the transaction_id, task_id are destroyed
   * there will also be a release_all_locks that will release all the locks and remove all the wait_entries for that transaction, and wake up all transactions waiting for the corresponding locks that we just released
   * effectively each task_id is a sequential steps to be executed and it is capable of putting it back into the sleep, and wake up when requested
   * task_id is 16 bit integer, and can be scheduled on anything, be it threadpool of the minitransaction engine OR the IO thread pool or the root threadpool of the transaction itself
   * transactions own locks not the task_ids

 1.* lock table (using VolatilePageStore)
   **** How can I let the lock_manager block the transaction, because it could be running inside a mini_transaction engine's threadpool of fixed size and blocking will block all other thread and possibly not letting the other transaction unblock it by releasing the locks. Additionally I can not let the transaction block before releasing the latches on pages and other structures, and the buffer pool etc and releasing other resources like cursors over the btree index and the heap table, I can not even go to wait before storing the information like the restart position of the scan for the btree or heap table or any other type of scan.
     * block outside, and use a callback to wake the blocking threads up, or queue the task to start a new scan form the tuple position in context in a heap table or btree or hash table
   ** ponder and ask chatgpt on a mechanism to release all latches before going to sleep on a condition variable to wait for a lock, we would also need to store the scan context for the b+tree or heap table scan position
   * decide on when to create and destroy active_transaction_entry, i.e. when to garbage collect
     * store counter to number of threads that went to wait for a transaction_id and the count of locks held, when they both become 0, remove that entry for the active transaction entry
   * decide the way to lock table and other components of the lock_manager
   * implement initialize and register_lock function of the lock_manager
   * outline the pseudocode for the acquire/transition and release function for the lock_manager
   * ponder how to signal about a potential deadlock, to the active_transaction_entry
     * set a bit stating that a deadlock was detected
   * ponder on how to implement release_all_locks function, and is there a way to do it with the existing functions with lesser time complexity?
     * will we need a function to get all locks for a transaction one by one (iterating with the tx_locks table) and wake up all blocked transaction and remove corresponding entries after that?
   * define basic lock martix for row, heap_table (ordering_lock), simple reader writer lock
   * write test cases and test

 2.* define catalog tables and indexes on it - persistent
   * same as indexes and heap_tables
   * prilimnary investigation revealed no-MVCC and no-per-tuple-locking, instead per table locking, do more research

 3.* giant_tuple_def, an in-memory tuple_def for an on-disk tuple_def (containing worms)
   * conversion function to convert on-disk tuple_def to giant_tuple_def
   * conversion function to convert on-disk tuple to giant_tuple
   * use this giant_tuple_def's tuples for in-memory tuples
   * compute max_size for this tuples using tuplestore, and also limit max element_count for the complete project as a macro

 4.* tuple_file structure
   * stores tuples or giant_tuples sequentially one after the another and reads them back like from a stream and deletes this temp file on closing
   * struct intermediate_tuple_store {
        pthread_mutex_t lock;
        uint64_t in_memory_size; // next_insert_offset crossing this value spill to the temp file
        uint64_t curr_file_size; // if non zero use fd instead
        union {
          int fd; // temp file, if next_insert_offset > in_memory_size
          void* md;
        }
        uint64_t next_insert_offset; -> more appropriate name append_offset
     }
   * struct tuple_region {
        uint64_t memory_block_offset_in_tuple_store; // always multiple of page size
        void* memory_block; // NULL if uninitialized
        uint64_t memory_block_size; // capped by curr_file_size

        void* tuple; // offset of this tuple in the memory = (tuple - memory_block)
        uint32_t tuple_size;
     } -> utility function to get memory_before_tuple, memory_at_tuple and memory_after_tuple in uint64_t bytes
   * void initializes_intermediate_tuple_store(intermediate_tuple_store* its_p, uint64_t in_memory_size)
   * void extend_intermediate_tuple_store(its_p, uint64_t additional_bytes); // additional_bytes over the next_insert_offset
     * any call to this function must take into account to release/unmap all tuple_regions
   * void* read_tuple(its_p, uin32_t* size);
   * void* map_tuple(intermediate_tuple_store* its_p, tuple_region* tr_p, uint64_t offset, const tuple_size_def* tpl_sz_d, uint64_t* next_tuple_offset);
   * void* wmap_tuple(intermediate_tuple_store* its_p, tuple_region* tr_p, uint64_t* offset, uint32_t* max_size);
     * on unmap and succeeding calls for the wmap, it updates the next_insert_offset
     * it maps all the memory at the end of the its_p to this tuple_region
     * if no mapped region provided then it starts mapping from the next_insert_offset
   * void unmap_tuple(intermediate_tuple_store* its_p, tuple_region* tr_p);
   * usefull for external sorting, hash joins and passing intermediate tuples between different operators

 5.0* implement SQL parser (possibly in a different directory)
 5.1* implement query plan tree
   * with operators
 5.2* implement relational algebra operators
   * selection -> filter with/without indexes
   * projection -> picking columns
   * sorting -> implement external giant_tuple_def sorting, by using a tuple_file
   * joins -> with/without indexes (hashjoins using a tuple_file)
   * union, set difference and intersection
    * Operators will be of two types
      * Scan -> run on mini transaction engine's thread pool, heap_table scan, index scan, etc
        -> accesses data from the database
        -> they accumulate tuples and push them when pulled
        -> they can be put to sleep and stopped at any instant and can be restarted, this checks are made every iteration
        -> they are source of the data
      * Transformers -> Operators that select, project, sort, join tuples -> they run on the transaction's thread pool or IO thread pool
        -> accesses data from the input stream to them, possibly a intermediate tuple store
        -> they just pull tuples, when their parent operator pulls on them
        -> they sleep and do nothing when we do not pull from these operators
        -> they are mere but smart transformers

 6.* define statistics tables - persistent and possibly in a different database
   * same as indexes and heap_tables (low priority)

 * Design methodology
   * we will use postgresql architecture over (mysql like) mini transactions
   * we will have heaps for storing tables, insert to which gives us a page_id (physical page_id) and a tuple_index (slot no) in that page
   * indexes will always include the page_id, tuple_index in the index key right after the actual key columns, and the index entry will be index key followed by the covering columns
   * standard latch rules apply index->heap_page, heap_pages_tree->heap_page
   * inserts/deletes will always be done to heap in a single mini transaction one at a time
     * inserts/deletes to indexes will be done in mini transactions not owned by anyone
   * updates are just delete followed by insert into the heap
   * reads will be be done without mini transactions, but in a fixed number of them
   * there will be vaccum, like postgresql
   * header will include xmin, xmax, is_xmin_committed?, is_xmax_committed?, is_xmin_aborted?, is_xmax_aborted?
     * xmin, xmax will be as wide as transaction_table suggests
     * the *? are advisory bitmaps and should be kept updated when ever a heap page is being read (if only a read lock on page is held then let another writer do it asynchronously) or written
   * we will never chain mini transactions for a high level transaction, we will just ensure that the transactions read only the data that is visible to them, and vaccum out invisible rows
   * lock_tables -> need to think over it