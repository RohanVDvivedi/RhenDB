 PICKED TASKS

 * conform to the new mini_transaction_engine allotment api and test

 1.* temp_tuple_store
   * implement functions use mmap64 only
   * code review
   * test

 2.* giant_tuple_def, an in-memory tuple_def for an on-disk tuple_def (containing worms)
   * conversion function to convert on-disk tuple_def to giant_tuple_def
   * conversion function to convert on-disk tuple to giant_tuple
   * use this giant_tuple_def's tuples for in-memory tuples
   * compute max_size for this tuples using tuplestore, and also limit max element_count for the complete project as a macro

 3.0* implement SQL parser (possibly in a different directory)
 3.1* implement query plan tree
   * with operators
 3.2* implement relational algebra operators
   * selection -> filter with/without indexes
   * projection -> picking columns
   * sorting -> implement external giant_tuple_def sorting, by using a tuple_file
   * joins -> with/without indexes (hashjoins using a tuple_file)
   * union, set difference and intersection
    * Operators will be of two types
      * Scan -> run on mini transaction engine's thread pool, heap_table scan, index scan, etc
        -> accesses data from the database
        -> they accumulate tuples and push them when pulled
        -> they can be put to sleep and stopped at any instant and can be restarted, this checks are made every iteration
        -> they are source of the data
      * Transformers -> Operators that select, project, sort, join tuples -> they run on the transaction's thread pool or IO thread pool
        -> accesses data from the input stream to them, possibly a intermediate tuple store
        -> they just pull tuples, when their parent operator pulls on them
        -> they sleep and do nothing when we do not pull from these operators
        -> they are mere but smart transformers

 4.* define catalog tables and indexes on it - persistent
   * stores only serialized immutable datatypes and persistent serialized immutable query_plans/functions
   * and an LRU cache on them
   * we will also need a clone function for the data_type_info-s and the query_plan-s

 5.* define statistics tables (*future) - persistent and possibly in a different database
   * same as indexes and heap_tables (low priority)

 6.* lock table remaining tasks (*future)
   * deadlock detection and resolution task job (*future, for now rely on timeouts to back off and abort)
   * define basic lock martix for row, heap_table (ordering_lock), simple reader writer lock (*future, do it when query optimizer and catalog tables are ready )

 * Design methodology
   * we will use postgresql architecture over (mysql like) mini transactions
   * we will have heaps for storing tables, insert to which gives us a page_id (physical page_id) and a tuple_index (slot no) in that page
   * indexes will always include the page_id, tuple_index in the index key right after the actual key columns, and the index entry will be index key followed by the covering columns
   * standard latch rules apply index->heap_page, heap_pages_tree->heap_page
   * inserts/deletes will always be done to heap in a single mini transaction one at a time
     * inserts/deletes to indexes will be done in mini transactions not owned by anyone
   * updates are just delete followed by insert into the heap
   * reads will be be done without mini transactions, but in a fixed number of them
   * there will be vaccum, like postgresql
   * header will include xmin, xmax, is_xmin_committed?, is_xmax_committed?, is_xmin_aborted?, is_xmax_aborted?
     * xmin, xmax will be as wide as transaction_table suggests
     * the *? are advisory bitmaps and should be kept updated when ever a heap page is being read (if only a read lock on page is held then let another writer do it asynchronously) or written
   * we will never chain mini transactions for a high level transaction, we will just ensure that the transactions read only the data that is visible to them, and vaccum out invisible rows
   * lock_tables -> need to think over it