 PICKED TASKS

 1.* lock table (using VolatilePageStore)
   * decide the way to lock table and other components of the lock_manager
   * implement initialize and register_lock function of the lock_manager
   * outline the pseudocode for the acquire/transition and release function for the lock_manager
   * ponder how to signal about a potential deadlock, to the active_transaction_entry
   * ponder on how to implement release_all_locks function, and is there a way to do it with the existing functions with lesser time complexity?
     * will we need a function to get all locks for a transaction one by one (iterating with the tx_locks table) and wake up all blocked transaction and remove corresponding entries after that?
   * define basic lock martix for row, heap_table (ordering_lock), simple reader writer lock
   * write test cases and test

 2.* define catalog tables and indexes on it - persistent
   * same as indexes and heap_tables
   * prilimnary investigation revealed no-MVCC and no-per-tuple-locking, instead per table locking, do more research

 3.* giant_tuple_def, an in-memory tuple_def for an on-disk tuple_def (containing worms)
   * conversion function to convert on-disk tuple_def to giant_tuple_def
   * conversion function to convert on-disk tuple to giant_tuple
   * use this giant_tuple_def's tuples for in-memory tuples
   * compute max_size for this tuples using tuplestore, and also limit max element_count for the complete project as a macro

 4.* tuple_file structure
   * stores tuples or giant_tuples sequentially one after the another and reads them back like from a stream and deletes this temp file on closing
   * reading can be done as a stream
   * write can also be done as a stream, returning the offset in the file for this tuple
   * file stays all in-memory up until a point, over which it spills to the disk, and then is accessed from there
   * no tuple updates allowed
   * also allows read-only mapping to mmap (MAP_SHARED and read-only mapping) pages containing a tuple, and operate on it as if it is all in memory
   * usefull for external sorting, hash joins and passing intermediate tuples between different operators

 5.0* implement SQL parser (possibly in a different directory)
 5.1* implement query plan tree
   * with operators
 5.2* implement relational algebra operators
   * selection -> filter with/without indexes
   * projection -> picking columns
   * sorting -> implement external giant_tuple_def sorting, by using a tuple_file
   * joins -> with/without indexes (hashjoins using a tuple_file)
   * union, set difference and intersection

 6.* define statistics tables - persistent and possibly in a different database
   * same as indexes and heap_tables (low priority)

 * Design methodology
   * we will use postgresql architecture over (mysql like) mini transactions
   * we will have heaps for storing tables, insert to which gives us a page_id (physical page_id) and a tuple_index (slot no) in that page
   * indexes will always include the page_id, tuple_index in the index key right after the actual key columns, and the index entry will be index key followed by the covering columns
   * standard latch rules apply index->heap_page, heap_pages_tree->heap_page
   * inserts/deletes will always be done to heap in a single mini transaction one at a time
     * inserts/deletes to indexes will be done in mini transactions not owned by anyone
   * updates are just delete followed by insert into the heap
   * reads will be be done without mini transactions, but in a fixed number of them
   * there will be vaccum, like postgresql
   * header will include xmin, xmax, is_xmin_committed?, is_xmax_committed?, is_xmin_aborted?, is_xmax_aborted?
     * xmin, xmax will be as wide as transaction_table suggests
     * the *? are advisory bitmaps and should be kept updated when ever a heap page is being read (if only a read lock on page is held then let another writer do it asynchronously) or written
   * we will never chain mini transactions for a high level transaction, we will just ensure that the transactions read only the data that is visible to them, and vaccum out invisible rows
   * lock_tables -> need to think over it