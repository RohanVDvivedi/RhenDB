 PICKED TASKS

  1.* heap_table
   * we will need 1 bplus_tree
     * key = (free_space, page_id) -> in ACID persistent store
   * this tree works just like any other index, except indexes point to (page_id, tuple_index) instead of just (page_id) from the tuple (free_space, page_id)
   * free space here is just an hint for the best page to accomodate a new tuple, it's will not always be consistent with the actual free_space on the page
   * additionally we will have a rwlock called ordering_lock, to be takes in shared mode to preserve and read a fixed ordering of the heap pages
   * it is taken in exclusive mode at periodic intervals to start small mini transactions to fix the free_space, if any of them is found to be wrong, during a scan or during the logic for finding the next best fit page for the new tuple
   * ordering lock is not required for finding free space for a tuple and for accessing pages after an index traversal
   * standard latch rules apply index->heap_page, heap_table->heap_page, heap_table is just like another index

 Design and api
   * separate module/directory heap_table
   * struct heap_table
   {
      uint64_t root_page_id;

      bplus_tree_tuple_defs heap_table_def;
      tuple_def* record_def;

      periodic_job* free_space_fixer;
      singlylist<pair<uint32_t free_space, uint64_t page_id>> free_space_fixer_params;
   };
   * fix_free_space_info_for_entry(uint64_t root_page_id, uint32_t free_space_old, uint64_t page_id); // removes 
   * get_new_heap_table_iterator(uint32_t free_space, uint64_t page_id, uint32_t tuple_index); // always scans in leaf-only read-locked traversal, heap_pages are the only ones that could be write locked here
   * get_curr_registered_free_space();
   * get_curr_page_id();
   * get_curr_heap_page(int write_locked);
   * get_curr_heap_tuple_index();
   * get_curr_heap_tuple();
   * next_entry_heap_table_iterator();
   * next_tuple_heap_table_iterator();
   * clone_heap_table_iterator();
   * delete_heap_table_iterator();
   * if you modify the heap_page please perform an insert into the free_space_fixer_params
   * possibly build it in tuple indexer, so that it can be used widely

 2.* define catalog tables and indexes on it - persistent
   * same as indexes and heap_tables

 3.0* implement SQL parser (possibly in a different directory)
 3.1* implement query plan tree
   * with operators
 3.2* implement relational algebra operators
   * selection -> filter with/without indexes
   * projection -> picking columns
   * sorting -> with/without indexes
   * joins -> with/without indexes
   * union, set difference and intersection

 3.* define statistics tables - persistent and possibly in a different database
   * same as indexes and heap_tables (low priority)

 4.* lock table (using VolatilePageStore)
 * attributes
   * resource_type - uint16_t
   * resource_id - uint256 -> table_id or row_id (page_id, tuple_index)
   * lock_mode (SHARED, EXCLUSIVE and many more) uint16_t
   * lock_status (WAITING or HELD) 1 bit
   * transaction_id (uint256 id of the transaction)
   * timestamp (creation of this entry, for wait timeouts) -> new entry to be created for lock_mode change from WAITING -> HELD
 * indexes
   * free_space_tree (free_space, page_id) => mandatory
   * transaction_id -> lock (to find all locks held and release them upon a commit or abort) => mandatory
   * resource_type, resource_id, lock_mode, lock_status -> lock (to find all contentions to acquire new locks AND to detect deadlocks)
   * think about more indexes as per accesses
 * this work can be delayed we can solely rely on mintxengine and MVCC rules to give us the snapshot isolation that we desire

 * Design methodology
   * we will use postgresql architecture over (mysql like) mini transactions
   * we will have heaps for storing tables, insert to which gives us a page_id (physical page_id) and a tuple_index (slot no) in that page
   * indexes will always include the page_id, tuple_index in the index key right after the actual key columns, and the index entry will be index key followed by the covering columns
   * standard latch rules apply index->heap_page, heap_pages_tree->heap_page
   * inserts/deletes will always be done to heap in a single mini transaction one at a time
     * inserts/deletes to indexes will be done in mini transactions not owned by anyone
   * updates are just delete followed by insert into the heap
   * reads will be be done without mini transactions, but in a fixed number of them
   * there will be vaccum, like postgresql
   * header will include xmin, xmax, is_xmin_committed?, is_xmax_committed?, is_xmin_aborted?, is_xmax_aborted?
     * xmin, xmax will be as wide as transaction_table suggests
     * the *? are advisory bitmaps and should be kept updated when ever a heap page is being read (if only a read lock on page is held then let another writer do it asynchronously) or written
   * we will never chain mini transactions for a high level transaction, we will just ensure that the transactions read only the data that is visible to them, and vaccum out invisible rows
   * lock_tables -> need to think over it