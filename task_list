 PICKED TASKS

  * find a way to compile a runtime function and it's validate counterpart from C or SQL code into binary and load it and run it

  * start with catalog tables (hash_table-s)
    * table for runtime_type -> name, data_type_info
    * table for runtime_function -> name to runtime_function both validate and function itself
    * table for disk_structures -> name, type (heap_table, hash_table, bplus_tree, linked_page_list, array_table, *r_tree), root_page_id, runtime_type.name, key_element_count, key_index_accessors(extended if required), sort_directions(extended if required), the whole tuple must fit 1/4th of the page
    * only hash_table and bplus_tree need key_element_count, and key_index_accessors, and sort_dirs are only needed by bplus_tree
    * data structures that do not need particular attributes may keep them empty or NULL
    * disk_structure_relationships (last 2 values are just page_id and tuple_index in the index's runtime_type, for uniqueness constraint)
      * table_name, index_name, key_index, how to get key from the tuple of the table => direct mapping or a runtime_function over the table's tuple
    * all of the tuple_defs for data_structures, type_infos and runtime_functions are all reference_counted and are read-only, they can not be modified
    * data_structure relation ships still can be queried

  * what operators to implement
    * scan on b+tree/hash index (sort rids (per 1 or set of buckets for hash index OR about some 1000 pages for bplus tree) and then scan the table), scan on table(start and move ahead linearly)
    * sort (external sorting, N-way)
    * joins (nested loop with per block, index nested loop, sort merge join, hash join)
    * group_by (based on hash or sorted input data otherwise)
    * read/write operators over simple data_structures (for full tuple indexer like access will be implemented later)
  * they need only 1 function to set them up called setup_hash_join, or setup_index_scan etc

  *implement various types of default runtime functions
   * boolean(convert NULL and ZERO to false and rest everything is true)
   * sign(negatives to -1, 0 to 0, and positives to 1)
   * compare(for comparing any types)
   * add, sub, mul, divide, mod, gcd, lcm, pow, sin, cos, multiplicate inverse, additive inverse
   * strings and blobs to be concatenated, substring with from and to
   * unix epoch to date, day, month, year, second, millisecond, microsecond, with timezones
   * build more complex functions transformers with local variables and bytecode stored in context
   * length of string, blob, array, tuple, etc

 1.0* implement SQL parser (possibly in a different directory)
 1.1* implement query plan tree
   * with operators
   * need functions to compare, and hash, including extended data types from tuple large types, two positons inside tuples or a tuple position and a materialized value, like materialized numeric
 1.2* implement relational algebra operators
   * use BoomPar's resource usage limiter to allot a mini transaction only if the buffer count it needs is available
   * every transaction starts with reservng 2 frames to access the transaction table
   * selection -> filter with/without indexes
   * projection -> picking columns
   * sorting -> implement external sorting, by using a temp_tuple_store
   * joins -> with/without indexes (hashjoins using a temp_tuple_store as buckets)
   * union, set difference and intersection

 PASS APPROPRIATE STACK SIZE, QUEUE MAX CAPACITY/TYPE AND WORKER COUNT FOR THE RHENDB THREADPOOLS

 2.* define catalog tables and indexes on it - persistent
   * stores only serialized immutable datatypes dti-s and things like bplus_tree_tuple_defs and persistent serialized immutable query_plans/functions and an LRU cache on them
   * there will also be reference counters for all of them, with LRU searches on them using name, index and their own pointers
   * they also point to each other, dtis pointing to other dtis in there and bplus_tree_tuple_defs and alike, pointing to record_defs that are dtis in there itself
   * functions and queries and functions can call other functions, reference them
   * we will use reference counting to already cached catalog objects, implement on_destroy and on_reference callbacks of type static_type_info_callback-s, that increment and decrement the reference counter for the dtis, that need to be passed around

 3.*maintain the inserts and deletes log as a stack in volatile page store and allow save points
   * savepoints will flip the is_xmin_NULL bit to 1 to undo an insert
   * and will flip the is_max_NULL bit to 1 to undo a delete
   * then we will do this until we hit a ROLLBACK to savepoint is hit
   * we will let vaccum do it's task of cleaning up inserts later in time
   * this stack will be in volatile page store and upon a crash we will just consider this transaction as aborted and we will never need this information

 4.* define statistics tables (*future) - persistent and possibly in a different database
   * same as indexes and heap_tables (low priority)

 5.* lock table remaining tasks (*future)
   * deadlock detection and resolution task job (*future, for now rely on timeouts to back off and abort)
   * define basic lock martix for row, heap_table (ordering_lock), simple reader writer lock (*future, do it when query optimizer and catalog tables are ready )

 * Design methodology
   * we will use postgresql architecture over (mysql like) mini transactions
   * we will have heaps for storing tables, insert to which gives us a page_id (physical page_id) and a tuple_index (slot no) in that page
   * indexes will always include the page_id, tuple_index in the index key right after the actual key columns, and the index entry will be index key followed by the covering columns
   * standard latch rules apply index->heap_page, heap_pages_tree->heap_page
   * inserts/deletes will always be done to heap in a single mini transaction one at a time
     * inserts/deletes to indexes will be done in mini transactions not owned by anyone
   * updates are just delete followed by insert into the heap
   * reads will be be done without mini transactions, but in a fixed number of them
   * there will be vaccum, like postgresql
   * header will include xmin, xmax, is_xmin_committed?, is_xmax_committed?, is_xmin_aborted?, is_xmax_aborted?
     * xmin, xmax will be as wide as transaction_table suggests
     * the *? are advisory bitmaps and should be kept updated when ever a heap page is being read (if only a read lock on page is held then let another writer do it asynchronously) or written
   * we will never chain mini transactions for a high level transaction, we will just ensure that the transactions read only the data that is visible to them, and vaccum out invisible rows
   * lock_tables -> need to think over it
